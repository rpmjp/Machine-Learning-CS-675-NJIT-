{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rpmjp/Machine-Learning-CS-675-NJIT-/blob/main/Week_3_Linear_separability%2C_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX3RsLoylGhZ"
      },
      "source": [
        "# **Module 4 &mdash; Perceptron and Linear Separability**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qaiRpkIlGhb"
      },
      "source": [
        "## **4.1 A Biological Neuron<font color='#FFCC00'>  &#x2B24; </font>**\n",
        "\n",
        "\n",
        "![Biological neuron illustration](https://docs.google.com/uc?export=download&id=1EPKYKcOpGw3hHTAlBw5wKfqiSWK5bOef)\n",
        "\n",
        "Biologists have studied animal neurons. A typical neuron works by taking a number of inputs that are chemical/electrical signals from other neurons through its dendrites. Not all incoming signals have the same 'weight': the brain is plastic and the dendrites modulate the signals before they are combined in the cell nucleus. When the combined modulated signals exceed some threshold (which may be different for each neuron), the neuron becomes *activated* and sends a signal to its output. Otherwise, the neuron is not activated.\n",
        "\n",
        "Since the neuron appears to have two fundamental states (activated or not), we can speculate that it may be able to make binary decisions or classifications. This leads us to the mathematical modeling of a biological neuron as a perceptron.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZaRCrkMlGhg"
      },
      "source": [
        "## **4.2 Perceptron<font color='#FF5733'> &#x2B24; </font>**\n",
        "\n",
        "We assume that the neuron/perceptron has $d$ inputs $x=[x_1,\\ldots,x_d]$ and each input is modulated, i.e. multiplied by a corresponding weight $w = [w_1,\\ldots,w_d]$. Thus what we call the **net input** is given by the following equation:\n",
        "\n",
        "$$\n",
        "    z = wx^T =  \\sum_{j=1}^d w_j x_j\\, .\n",
        "$$\n",
        "\n",
        "The neuron then has an **activation function** $\\phi(z)$ defined as follows for some activation threshold $\\theta\\,$:\n",
        "\n",
        "\n",
        "*   $\\phi(z) =1$, if $z\\geq \\theta\\:$ **or**  $\\:\\phi(z) = 0$, if $z<\\theta$ .\n",
        "\n",
        "Another way to state the above rule is to set $b = -\\theta$ and write it as follows:\n",
        "*   $\\phi(z) =1$, if $wx^T + b\\geq 0\\:$ **or**  $\\:\\phi(z) = 0$, if $wx^T+b< 0$ .\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "**Some additional notation**. For conciseness, we can set $w_0=b$ and let $w = [w_0,w_1,\\ldots,w_d]$. Then, we can assume that all our points have an extra 'fixed' coordinate and they look like this: $x=[1,x_1,\\ldots, x_d]$. In that case the rule becomes:\n",
        "\n",
        "*   $\\phi(z) =1$, if $wx^T \\geq 0\\:$ **or**  $\\:\\phi(z) = 0$, if $wx^T< 0$ .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3 Linear Separability<font color='#FF5733'> &#x2B24; </font>**\n",
        "\n",
        "It is very useful to think about the points $x$ at which the perceptron is undecided, i.e. *on the fence* between the two decisions.\n",
        "Working with the definition, these are exactly the set of points $x$ that satisfy\n",
        "\n",
        "$$ wx^T+ b =0\\, .$$\n",
        "\n",
        "This decision boundary, or **decision surface**, is the equation of a $d$-dimensional hyperplane, and the two decision regions will be what we call half-spaces.\n",
        "\n",
        "![linear separatility in 2D](https://docs.google.com/uc?export=download&id=1x9RwkW-cQxfMo7_G3oV6KermBVUjxpXI)\n",
        "\n",
        "For a 2D example, the boundary is a line, while the two semi-planes are the two decision regions.\n",
        "\n",
        "This leads us to the notion of **linear separability**: If we have a dataset $X$ where datapoints belong to two classes, and these two classes can be separated by a hyperplane, then we say that $X$ is linearly separable.\n",
        "\n",
        "It follows, by definition, that if $X$ is linearly separable then there exists a perceptron that can separate it (and be 100% accurate on all points). And vice-versa, if $X$ is not linearly separable, then no perceptron can fully separate it.\n"
      ],
      "metadata": {
        "id": "1MxwZjtK7lv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.4  The Perceptron Algorithm<font color='#008DFF'> &#x2B24; </font>**\n",
        "\n",
        "We now want to come up with an algorithm that will learn the weights $w,b$ of a perceptron, so as to separate the classes of $X$ when $X$ is linearly separable. Recall that $X$ comes with a set of categorical labels $y$, and in this case we can set these labels to the numerical values $0$, $1$ so that they coincide with the possible outputs of the perceptron.\n",
        "\n",
        "Here is the algorithm:\n",
        "\n",
        "---\n",
        "*   **function** $w$ = Perceptron-fit ($X$,$y$)\n",
        "*   Randomly initialize a $d$-dimensional vector $w$ and a scalar $b$\n",
        "*   $\\mathit{errorFlag} = \\mathrm{True}$\n",
        "*   **while** $\\mathit{errorFlag}$ **do**    &nbsp; &nbsp; <font color='#008DFF'> #new epoch starts here </font>\n",
        "*   &nbsp; &nbsp; &nbsp; $\\mathit{errorFlag} = \\mathrm{False}$\n",
        "*   &nbsp; &nbsp; &nbsp; **for** $j=1$ **to** $n$:\n",
        "*   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $z = b + X_j w^T$\n",
        "*   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **if** &nbsp; $z\\geq 0$ &nbsp; **then** &nbsp; $\\hat{y} = 1$ &nbsp; **else** &nbsp; $\\hat{y} = 0$  &nbsp; &nbsp;  <font color='#008DFF'> #in linear regression we use $\\hat{y} = z$ </font>\n",
        "*   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **if** &nbsp; $\\hat{y} \\neq  y_j$ &nbsp; **then** &nbsp; $\\mathit{errorFlag} = \\mathrm{True}$\n",
        "*   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $b  = b -  \\left( \\hat{y}  - y_j\\right ) $\n",
        "*   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **for** $k=1$ **to** $d$:  \n",
        "*   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
        "     $w_k = w_k - \\left( \\hat{y} - y_j\\right) X_{j,k} $ &nbsp;\n",
        "*   **return** $w,b$\n",
        "---\n",
        "\n",
        "The algorithm strategy is to learn from each mistake. When the perceptron makes a mistake on a point $X_j$, then the weights $w,b$ are updated, and in fact the update rules are precisely the rules we used in stochastic gradient descent (with $\\rho = 1$) for linear regression with one exception: $\\hat{y}$ is now a categorical value $\\{0,1\\}$, whereas in linear regression we were using $\\hat{y}=z$.\n",
        "\n",
        "Like gradient descent, many variants of the perceptron algorithm are possible, with different learning rates $\\rho$, stochastic variants and other variants involving batching, and so on. Note also that the algorithm can be run with other numerical values as class labels (such as $y,\\hat{y}\\in\\{-1,1\\}$).\n",
        "\n",
        "The algorithm stops only when no $\\mathit{errorFlag}$ is raised during an epoch, i.e. when all points are classified correctly. But is that guaranteed to happen?\n",
        "\n",
        "Yes, as it turns out.\n",
        "\n",
        "**Fact 1:** If $X$ is a linearly separable dataset, then the perceptron algorithm will eventually terminate.\n",
        "\n",
        "There is an even stronger statement that can be made.\n",
        "\n",
        "**Fact 2:** If $X$ is a linearly separable dataset, then one can compute a number $M$ that depends only on $X$, such that the perceptron algorithm will make at most $M$ mistakes during its course.\n",
        "\n",
        "Fact 2 is known as the Rosenblatt-Novikoff Theorem. Frank Rosenblatt was the person that in 1958 came up with the idea of the perceptron, which according to many paved the way for modern Machine Learning [[1](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon)]. You can find a proof and some other cool things about the perceptron in this [link](http://ciml.info/dl/v0_99/ciml-v0_99-ch04.pdf).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L85s3Up4s1Tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.5  One-vs-One for Multilabel Classification<font color='#FFCC00'> &#x2B24; </font>**\n",
        "\n",
        "We now introduce the One-vs-One method for using perceptrons on multilabel data.\n",
        "\n",
        "We will use a small example. Suppose that our data $X$ contains points with three labels: *dog*, *cat*, *human*. With this labeling, we can think of $X$ being partitioned into 3 submatrices: $X_{\\mathit{cat}}$, $X_{\\mathit{dog}}$, $X_{\\mathit{human}}$. For each pair of data classes, we can train a perceptron. More specifically, we train the following classifiers:\n",
        "\n",
        "* $C_1$, the cat-vs-dog classifier, using only data in $X_{\\mathit{cat}}$ and $X_{\\mathit{dog}}$.\n",
        "* $C_2$, the human-vs-dog classifier, using only data in $X_{\\mathit{human}}$ and $X_{\\mathit{dog}}$.\n",
        "* $C_3$, the human-vs-cat classifier, using only data in $X_{\\mathit{human}}$ and $X_{\\mathit{cat}}$.\n",
        "\n",
        "Then, if we want to predict the label of a new point $x$, we first generate three different predictions from the classifiers $C_1$, $C_2$, $C_3$.  Then we imagine a 'voting' process where each classifier votes for dog, human, or cat. The prediction that takes the most votes is the final prediction for $x$. If we have a tie, then we break it arbitrarily between the winners.\n",
        "\n",
        "More generally, if we have data classes then we need to train ${k \\choose 2} = k(k-1)/2$ different classifiers and use the majority rule."
      ],
      "metadata": {
        "id": "O13vCy9qEtcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.6  Learning Logical Gates<font color='#FFCC00'> &#x2B24; </font>**\n",
        "\n",
        "Computer hardware and CPUs in particular, consist of millions of simple transistors implementing three basic logical functions: AND, OR and NOT.\n",
        "\n",
        "![Truth tables for logical gates](https://docs.google.com/uc?export=download&id=1wRIUQyytudayzx9S94nGr0LaK4SKsR5T)\n",
        "\n",
        "All other logical functions can be impemented using networks of these three basic gates. For example the XOR function (whose truth table is above) can be implemented as a 3-layer network of NOT-AND gates as shown here:\n",
        "\n",
        "![XOR implemented with NAND gates](https://docs.google.com/uc?export=download&id=1wX8S_kHuIxpsVLOzJ5JiQA3NXxXvkjoY)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EKYyOBpFhL_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "One way to see these logical functions (say the AND gate) is as a binary classifier that separates 4 points into two classes. Then one can ask this question: Can a perceptron 'learn' the 2-input OR gate? We can instantly see that the answer is yes by looking at the linear separability in the picture. Similarly, AND & NOT-AND are linearly separable.\n",
        "\n",
        "![Linear separability for logical gates](https://docs.google.com/uc?export=download&id=1wguGVcgldwMEa1RobiwH_wgyBqxZB6Z4)\n",
        "\n",
        "On the other hand, the XOR logical gate cannot be learned by a simple perceptron, but as we will see we a network of perceptrons can 'learn' the XOR gate.\n",
        "\n",
        "Similarly to logical circuits in computers , it is believed that the high-level functionality of our brain emerges through networks of neurons. But what type of computational tasks can individual neurons perform? It was until recently believed that individual human neurons can only behave like simple separable logical gates, but there is now evidence that single neurons can actually be more complex and operate even like XOR gates [[2](https://www.quantamagazine.org/neural-dendrites-reveal-their-computational-power-20200114/)]."
      ],
      "metadata": {
        "id": "OnH9HOojOkDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.6  Perceptron Code Notebook<font color='#369870'> &#x2B24; </font>**\n",
        "\n",
        "Here is the [code notebook](https://colab.research.google.com/drive/1aXWNLI5tIIWcbrr6mSk_YzO7CRWKul5Q?usp=drive_fs)."
      ],
      "metadata": {
        "id": "7jGVR6Ns8tXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.7  Feature Engineering Revisited<font color='#FFCC00'> &#x2B24; </font>**\n",
        "\n",
        "Consider the picture on the left. The dataset is clearly not separable, so it can not be learned by a perceptron.\n",
        "\n",
        "To each point $(x,y)$ in our dataset, we now add one extra feature/coordinate, $x^2+y^2$, so that the point becomes $(x,y,x^2+y^2)$.\n",
        "\n",
        "![The non-linear power of engineered features](https://docs.google.com/uc?export=download&id=1B-SBFNy3OgR0XTkEWYTW8RgBHCWMc_eC)\n",
        "\n",
        "This 'engineered' 3D-dataset is plotted in the second figure. We can see that the dataset has now become linearly separable. Specifically we see that the plane separating the two classes is defined by $x^2+y^2 = 0.5$; that is, the third coordinate is equal to 0.5. If we go back to the original space, the set of points $(x,y)$ that satisfy the equation $x^2+y^2 = 0.5$ is a circle of radius $1/\\sqrt{2}$. The linear separation surface in 3D becomes a non-linear surface in 2D.  \n",
        "\n",
        "This is yet another example of the power of feature engineering. However, generally speaking, engineering appropriate features is a difficult task that requires a very good understanding of the application and/or the dataset.\n"
      ],
      "metadata": {
        "id": "3xpuSiTofvOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.8 Perceptron Weaknesses<font color='#FFCC00'> &#x2B24; </font>**\n",
        "\n",
        "We can now identify some weaknesses of the perceptron:\n",
        "\n",
        "* Given a linearly separable dataset $X$, the number of perceptrons that can separate the classes is usually infinite. The output of the perceptron learning algorithm depends on the random initialization and the order in which the points are visited by the algorithm.\n",
        "* In some cases, the algorithm can learn a perceptron with a decision surface that is much closer to one of the classes than it is to the other. That is demonstrated in the following figure. Intuitively speaking, such decision surfaces are undesirable since we should not have any *a priori* preference for any class.\n",
        "\n",
        "\n",
        "\n",
        "![A linearly separable dataset](https://docs.google.com/uc?export=download&id=1x9RwkW-cQxfMo7_G3oV6KermBVUjxpXI)"
      ],
      "metadata": {
        "id": "93PB8ZRpxl5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **References**\n",
        "\n",
        "[1] [Professor's perceptron paved the way for AI &mdash; 60 years too soon](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon) <br>\n",
        "[2] [Hidden computational power found in the arms of neurons](https://www.quantamagazine.org/neural-dendrites-reveal-their-computational-power-20200114/)"
      ],
      "metadata": {
        "id": "2WO1zrS7TZ4q"
      }
    }
  ]
}